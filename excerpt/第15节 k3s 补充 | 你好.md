> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [docker.nsddd.top](https://docker.nsddd.top/Cloud-Native-k8s/15.html#%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E9%80%89%E9%A1%B9)

> 链学社致力于打造出区块链去中心化的学习平台

*   [authoropen in new window](http://nsddd.top/)

  

> ❤️💕💕新时代拥抱云原生，云原生具有环境统一、按需付费、即开即用、稳定性强特点。Myblog:[http://nsddd.topopen in new window](http://nsddd.top/)

* * *

*   [资源分析](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E8%B5%84%E6%BA%90%E5%88%86%E6%9E%90)
    *   [/usr/local/bin 重要二进制](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#usr-local-bin-%E9%87%8D%E8%A6%81%E4%BA%8C%E8%BF%9B%E5%88%B6)
*   [脚本安装选项](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E9%80%89%E9%A1%B9)
    *   [总结](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E6%80%BB%E7%BB%93)
*   [对二进制的安装高级补充](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E5%AF%B9%E4%BA%8C%E8%BF%9B%E5%88%B6%E7%9A%84%E5%AE%89%E8%A3%85%E9%AB%98%E7%BA%A7%E8%A1%A5%E5%85%85)
*   [通过配置文件启动 K3s](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E9%80%9A%E8%BF%87%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%90%AF%E5%8A%A8-k3s)
*   [K3s Server/Agent 配置](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#k3s-server-agent-%E9%85%8D%E7%BD%AE)
*   [网络选项](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E7%BD%91%E7%BB%9C%E9%80%89%E9%A1%B9)
    *   [Flannel 选项](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#flannel-%E9%80%89%E9%A1%B9)
    *   [flannel-backend 使用 host-gw](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#flannel-backend-%E4%BD%BF%E7%94%A8-host-gw)
    *   [启用 Directrouting](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E5%90%AF%E7%94%A8-directrouting)
*   [自定义 CNI](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E8%87%AA%E5%AE%9A%E4%B9%89-cni)
*   [使用外部数据库实现高可用安装](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E4%BD%BF%E7%94%A8%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%89%E8%A3%85)
    *   [环境准备](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87)
    *   [外部数据库高可用](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E9%AB%98%E5%8F%AF%E7%94%A8)
    *   [agent 加入](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#agent-%E5%8A%A0%E5%85%A5)
    *   [没有 CLI 标志启动 agent 加入](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E6%B2%A1%E6%9C%89-cli-%E6%A0%87%E5%BF%97%E5%90%AF%E5%8A%A8-agent-%E5%8A%A0%E5%85%A5)
*   [嵌入式 DB HA](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E5%B5%8C%E5%85%A5%E5%BC%8Fdb-ha)
*   [集群数据存储选项](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E9%80%89%E9%A1%B9)
    *   [配置参数](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0)
*   [私有仓库](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93)
    *   [registries.yaml 文件](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#registries-yaml-%E6%96%87%E4%BB%B6)
    *   [配置 Containerd](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E9%85%8D%E7%BD%AE-containerd)
    *   [将映像添加到专用注册表](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E5%B0%86%E6%98%A0%E5%83%8F%E6%B7%BB%E5%8A%A0%E5%88%B0%E4%B8%93%E7%94%A8%E6%B3%A8%E5%86%8C%E8%A1%A8)
*   [离线安装](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85)
    *   [通过私有镜像仓库安装 K3s](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E9%80%9A%E8%BF%87%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E5%AE%89%E8%A3%85-k3s)
*   [升级 K3s](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E5%8D%87%E7%BA%A7-k3s)
    *   [通过脚本升级](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E9%80%9A%E8%BF%87%E8%84%9A%E6%9C%AC%E5%8D%87%E7%BA%A7)
    *   [在线脚本升级](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E5%9C%A8%E7%BA%BF%E8%84%9A%E6%9C%AC%E5%8D%87%E7%BA%A7)
    *   [Channels 说明](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#channels-%E8%AF%B4%E6%98%8E)
    *   [使用安装脚本升级 K3s](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E4%BD%BF%E7%94%A8%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%E5%8D%87%E7%BA%A7-k3s)
    *   [自动升级](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E8%87%AA%E5%8A%A8%E5%8D%87%E7%BA%A7)
*   [连接到 k3s kubernets 集群的三种方式](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#%E8%BF%9E%E6%8E%A5%E5%88%B0-k3s-kubernets-%E9%9B%86%E7%BE%A4%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F)
    *   [kubeconfig](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#kubeconfig)
    *   [kubectl](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#kubectl)
    *   [Lens Kubernetes IDE](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#lens-kubernetes-ide)
*   [END 链接](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/Cloud-Native-k8s/15.html#end-%E9%93%BE%E6%8E%A5)

[TOC]

[#](#资源分析) 资源分析
---------------

资源占用率低是 k3s 突出的特点，针对 k3s 特性，分析资源的占用：

**影响资源利用率的因素：**

*   `K3s server`：K3s server 的利用率数据主要是由支持 Kubernetes 数据存储（kine 或 etcd）、API Server、Controller-Manager 和 Scheduler 控制。 **创建 / 修改 / 删除** 资源将导致暂时的利用率上升。大量使用 Kubernetes 数据存储的 operators 或应用程序也将增加 server 的资源需求。
    
*   `K3s agent`：管理镜像、提供 **存储或创建 / 销毁容器** 的操作将导致利用率的暂时上升，拉取镜像通常会影响 CPU 和 IO，因为它们涉及将镜像内容解压到磁盘。如果可能的话，工作负载存储 (pod 临时存储和卷) 应该与 agent 组件( `/var/lib/rancher/k3s/agent` ) 隔离，以确保不会出现资源冲突。
    

**防止 agent 和工作负载干扰集群数据存储：**

*   在 `server` 节点运行工作负载 pod 时，应确保 `agent` 和工作负载 `IOPS` 不干扰数据存储。
    
*   将 `server` 组件（`/var/lib/rancher/k3s/server`）与 `agent` 组件（`/var/lib/rancher/k3s/agent`）放在不同的存储介质上，后者包括 `containerd` 镜像存储。
    
*   工作负载存储（`pod` 临时存储和卷）也应该与数据存储隔离。
    

### [#](#usr-local-bin-重要二进制) /usr/local/bin 重要二进制

提示

后面的测试都是和 `/usr/local/bin` 息息相关，就比如说每一次测试删除：

```
/usr/local/bin/k3s-uninstall.sh 


```

**同样的还有停止 k3s：**

为了在升级期间实现高可用性，K3s 容器在 K3s 服务停止时会继续运行。

```
/usr/local/bin/k3s-killall.sh


```

killall 脚本能清理容器、K3s 目录和网络组件，同时还能删除 iptables 链以及所有相关规则。集群数据不会被删除。

🏄‍♂️ 同样可以使用 `systemctl start k3s` ，目前发现效果一样，如果你觉得不一样，联系下我~

**当然 k3s 都是围绕着 k3s 脚本为中心的：**

[#](#脚本安装选项) 脚本安装选项
-------------------

提示

我们现在已经知道了关于脚本的选项两种方式，**环境变量** 或者 **标志** 。

⚠️ 注意，如果你选择 **下载 install.sh** 使用 https://ghproxy.com ，后面的所有脚本我都是使用：

```
INSTALL_K3S_MIRROR=cn   INSTALL_K3S_SYMLINK=skip  sh install.sh


```

以 "K3S_" 开头的环境变量将被保留，供 systemd 和 openrc 服务使用。

在没有明确设置 exec 命令的情况下设置`K3S_URL`，会将命令默认为 "agent"。

运行 agent 时还必须设置`K3S_TOKEN`。

`INSTALL_K3S_SKIP_DOWNLOAD` -- (用于离线安装) 如果设置为 "true" 将不会下载 K3s 的哈希值或二进制。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_SKIP_DOWNLOAD=true \
    sh -


```

`INSTALL_K3S_SYMLINK` -- 默认情况下，如果路径中不存在命令，将为 `kubectl`、`crictl` 和 `ctr` 二进制文件创建符号链接。如果设置为 `'skip'` 将不会创建符号链接，而 `'force'` 将覆盖。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_SYMLINK=skip \
  sh -


```

> **测试 (默认情况）：**
> 
> ```
> root@etcnode01:/usr/local/bin
> total 66164
> drwxr-xr-x  2 root root     4096 Nov 26 09:40 .
> drwxr-xr-x 10 root root     4096 Aug 31 06:52 ..
> lrwxrwxrwx  1 root root        3 Nov 26 06:19 crictl -> k3s
> -rwxr-xr-x  1 root root 67735552 Nov 26 06:19 k3s
> -rwxr-xr-x  1 root root     1433 Nov 26 06:19 k3s-agent-uninstall.sh
> -rwxr-xr-x  1 root root     2026 Nov 26 06:19 k3s-killall.sh
> lrwxrwxrwx  1 root root        3 Nov 26 06:19 kubectl -> k3s
> 
> 
> ```
> 
> **测试（指定环境变量）：**
> 
> ```
> root@cubmaster01:/usr/local/bin
> total 66168
> drwxr-xr-x  2 root root     4096 Nov 26 11:16 .
> drwxr-xr-x 10 root root     4096 Aug 31 06:52 ..
> -rwxr-xr-x  1 root root 67735552 Nov 26 11:16 k3s
> -rwxr-xr-x  1 root root     2026 Nov 26 11:16 k3s-killall.sh
> -rwxr-xr-x  1 root root     1397 Nov 26 11:16 k3s-uninstall.sh
> 
> 
> ```

注意

⚠️ 我在上一节介绍了使用 **别名** ，它可以派上用场了。

`INSTALL_K3S_SKIP_ENABLE` -- 如果设置为 "true"，将不启用或启动 K3s 服务。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_SKIP_ENABLE=true \
  sh -


```

> **开启：**
> 
> ```
> systemctl start k3s
> kubectl get nodes
> 
> 
> ```

`INSTALL_K3S_SKIP_START` -- 如果设置为 "true" 将不会启动 K3s 服务。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_SKIP_START=true \
  sh -


```

`INSTALL_K3S_VERSION` -- 从 Github 下载 K3s 的版本。如果没有指定，将尝试从 "stable" 频道下载。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_VERSION="v1.19.9+k3s1" \
  sh -


```

> 注意后面是 `+`

`INSTALL_K3S_BIN_DIR` -- 安装 K3s 二进制文件、链接和卸载脚本的目录，或者使用 `/usr/local/bin` 作为默认目录。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_BIN_DIR=/opt/bin \
  sh -


```

警告

这里有个坑，它給设置 二进制文件路径，但是不给改变环境变量路径~

一个 idea ：如果我们创建的 hash 目录，我们是不是可以将 roofs 设置为环境变量，就比如说 `cd ~k3s` OR `cd $K3S`

我们只需要拿到 hash 路径 `export K3S="/var/lib/rancher/k3s/data/${HASH}/bin"`

`INSTALL_K3S_BIN_DIR_READ_ONLY` -- 如果设置为 true 将不会把文件写入 INSTALL_K3S_BIN_DIR，强制设置 INSTALL_K3S_SKIP_DOWNLOAD=true。

`INSTALL_K3S_SKIP_DOWNLOAD` 会创建 `kubectl/crictl/ctr` 等，而 `INSTALL_K3S_BIN_DIR_READ_ONLY` 不创建。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_BIN_DIR_READ_ONLY=true \
  sh -


```

`INSTALL_K3S_SYSTEMD_DIR` -- 安装 systemd 服务和环境文件的目录，或者使用 / etc/systemd/system 作为默认目录。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_SYSTEMD_DIR=/opt/systemd \
  sh -


```

测试

```
root@cubmaster01:/workspces/runtime
root@cubmaster01:/opt/systemd
total 12
drwxr-xr-x 2 root root 4096 Nov 26 11:53 .
drwxr-xr-x 6 root root 4096 Nov 26 11:51 ..
-rw-r--r-- 1 root root  829 Nov 26 11:53 k3s.service
-rw------- 1 root root    0 Nov 26 11:53 k3s.service.env


```

`INSTALL_K3S_EXEC` -- 带有标志的命令，用于在服务中启动 K3s。**如果未指定命令，并且设置了 K3S_URL，它将默认为 “agent”。如果未设置 K3S_URL，它将默认为 “server”。**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC="--docker" \
  sh -


```

** 最后的 systemd 命令解析为这个环境变量和脚本参数的组合。** 为了说明这一点，以下命令的结果与注册一个没有 flannel 的 server 的行为相同：

```
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--flannel-backend none" sh -s -
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --flannel-backend none" sh -s -
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server" sh -s - --flannel-backend none
curl -sfL https://get.k3s.io | sh -s - server --flannel-backend none
curl -sfL https://get.k3s.io | sh -s - --flannel-backend none


```

`INSTALL_K3S_NAME` -- 要创建的 `systemd` 服务名称，如果以服务器方式运行 k3s，则默认为 `'k3s'`；如果以 `agent` 方式运行 `k3s`，则默认为 `'k3s-agent'` 。如果指定了服务名，则服务名将以 `'k3s-'` 为前缀。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_ \
  sh -


```

`INSTALL_K3S_TYPE` -- 要创建的 systemd 服务类型，默认为 notify

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_TYPE="exec" \
  sh -


```

`INSTALL_K3S_SKIP_SELINUX_RPM` -- 如果设置为 `"true "` 将跳过 `k3s RPM` 的自动安装。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_SKIP_SELINUX_RPM=true \
  sh -


```

K3s RPM

[安全增强的 Linux（SELinux）open in new window](https://en.wikipedia.org/wiki/Security-Enhanced_Linux) 是对 Linux 的安全增强。

它由 Red Hat 开发，是 Linux 上强制性访问控制（MAC）的一个实现。强制性访问控制允许系统管理员定义应用程序和用户如何访问不同的资源，如文件、设备、网络和进程间通信。SELinux 还通过使操作系统在默认情况下具有限制性而增强了安全性。

在历史上被政府机构使用后，SELinux 现在是行业标准，在 CentOS 7 和 8 上默认启用。要检查 SELinux 是否在你的系统上启用和执行，请使用`getenforce`。

`INSTALL_K3S_CHANNEL_URL` -- 用于获取 K3s 下载网址的频道 URL。默认为 https://update.k3s.io/v1-release/channels 。

`INSTALL_K3S_CHANNEL` -- 用于获取 K3s 下载 URL 的通道。默认值为 "stable"。选项包括：`stable`, `latest`, `testing`。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_CHANNEL="latest" \
  sh -


```

`K3S_CONFIG_FILE` -- 指定配置文件的位置。默认目录为`/etc/rancher/k3s/config.yaml`。

我们先指定下文件：

```
cat >> /opt/config.yaml <<-EOF
node-label:
- "foo=bar"
- "something=amazing"
EOF


```

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  K3S_CONFIG_FILE=/opt/config.yaml \
  sh -


```

![](http://sm.nsddd.top/smimage-20221126211751303.png)

`K3S_TOKEN` -- 用于将 server 或 agent 加入集群的共享 secret。

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  K3S_TOKEN=rancher-k3s \
  sh -


```

希望你可以找到它的位置

```
cat /var/lib/rancher/k3s/server/token
K1042465c14be8de6a57c482b4162f673addcb652acb13c8119a9900b5d27c234f7::server:rancher-k3s


```

`K3S_TOKEN_FILE` -- 指定 `cluster-secret`,`token` 的文件目录。

我们需要先指定文件

```
cat >> /opt/token.txt <<-EOF
rancher-k3s
EOF


```

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  K3S_TOKEN_FILE=/opt/token.txt \
  sh -


```

希望你可以找到它的位置

```
root@cubmaster01:/workspces/runtime
K10fbe884032e42976a1dd419e5d171b9bc38d50e13dc852afbc97ffb99c765f06e::server:rancher-k3s


```

### [#](#总结) 总结

提示

1.  以 "K3S_" 开头的环境变量将被保留，供 systemd 和 openrc 服务使用。
2.  在没有明确设置 exec 命令的情况下设置 K3S_URL，会将命令默认为 "agent"。
3.  运行 agent 时还必须设置 K3S_TOKEN。

[#](#对二进制的安装高级补充) 对二进制的安装高级补充
-----------------------------

安装脚本主要是配置 K3s 作为服务运行。如果你选择不使用脚本，你可以通过 [发布页面 open in new window](https://github.com/rancher/k3s/releases/latest) 下载二进制文件，将其放在你的环境变量路径上，然后执行它来运行 K3s。K3s 二进制支持以下命令：

`k3s server` -- 运行 K3s server，它还将启动 Kubernetes control-plane 组件，如 API server, controller-manager, 和 scheduler。

> 和 kubernetes 的控制层面很类似~

```
root@cubmaster01:/workspces/runtime
INFO[0000] Acquiring lock file /var/lib/rancher/k3s/data/.lock 
INFO[0000] Preparing data dir /var/lib/rancher/k3s/data/2ef87ff954adbb390309ce4dc07500f29c319f84feec1719bfb5059c8808ec6a 
INFO[0000] Starting k3s v1.25.3+k3s1 (f2585c16)         
INFO[0000] Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s 
INFO[0000] Configuring database table schema and indexes, this may take a moment... 
INFO[0000] Database tables and indexes are up to date   
INFO[0000] Kine available at unix://kine.sock 
......


```

`k3s agent` -- 运行 `K3s agent` 节点。这将使 `K3s` 作为工作节点运行，启动 `Kubernetes` 节点服务 `kubelet` 和 `kube-proxy`。

```
root@etcnode01:~# k3s agent --server https://192.168.71.130:6443 --token K10fcaced71fc70ca6b77921a7e374dc03c34fdd1fb11973d69a2a8e937b61beb22::server:82c397ed440c496f5448ec3c4b11c112
INFO[0000] Starting k3s agent v1.25.4+k3s1 (0dc63334)   
INFO[0000] Running load balancer k3s-agent-load-balancer 127.0.0.1:6444 -> [192.168.71.130:6443] 
ERRO[0004] failed to get CA certs: Get "https://127.0.0.1:6444/cacerts": read tcp 127.0.0.1:60386->127.0.0.1:6444: read: connection reset by peer 
......


```

> ```
> root@cubmaster01:/opt
> NAME          STATUS   ROLES                  AGE   VERSION
> cubmaster01   Ready    control-plane,master   96m   v1.25.3+k3s1
> cubnode02     Ready    <none>                 52m   v1.25.4+k3s1
> etcnode01     Ready    <none>                 67m   v1.25.4+k3s1
> 
> 
> ```

`k3s kubectl` -- 运行嵌入式 kubectl CLI。如果没有设置 KUBECONFIG 环境变量，当启动 K3s 服务器节点时，将自动尝试使用在`/etc/rancher/k3s/k3s.yaml` 创建的配置文件。

```
root@k3s1:~
NAME   STATUS   ROLES                  AGE     VERSION
k3s1   Ready    control-plane,master   8m14s   v1.20.5+k3s1
k3s2   Ready    <none                11s     v1.20.5+k3s1


```

`k3s crictl` -- 运行一个嵌入式 crictl。这是一个用于与 Kubernetes 的容器运行时接口（CRI）交互的 CLI。对调试很有用。

```
root@k3s1:~
CONTAINER           IMAGE               CREATED             STATE               NAME                     ATTEMPT             POD ID
9ceb610df16c7       aa764f7db3051       8 minutes ago       Running             traefik                  0                   373c79416fa65
cadceb62ae08d       897ce3c5fc8ff       8 minutes ago       Running             lb-port-443              0                   f8a0ecfe56562
a26a49be485ac       897ce3c5fc8ff       8 minutes ago       Running             lb-port-80               0                   f8a0ecfe56562
01894072f2298       148c192562719       8 minutes ago       Running             local-path-provisioner   1                   b9d55e63f632f
5ccd6ed05120f       296a6d5035e2d       9 minutes ago       Running             coredns                  0                   2bae007d8e486
be0765e77a703       9dd718864ce61       9 minutes ago       Running             metrics-server           0                   53ab949c026ce


```

`k3s ctr` -- 运行一个嵌入式的 ctr。这是为 containerd（K3s 使用的容器守护进程）提供的 CLI。对调试很有用。

```
root@k3s1:~
CONTAINER                                                           IMAGE                                                                                                               RUNTIME
01894072f2298208f3c109f9fb1d5e12e677d11cd5d0b0a3a66f550ae38644e4    docker.io/rancher/local-path-provisioner:v0.0.19                                                                    io.containerd.runc.v2
2bae007d8e486afffbbf1ffb88e97b92d367aff4b06842217de4fb5d22ecf1b9    docker.io/rancher/pause:3.1


```

`k3s server` 和 `k3s agent` 命令有额外的配置选项，可以通过 `k3s server --help` 或 `k3s agent --help` 查看。

[#](#通过配置文件启动-k3s) 通过配置文件启动 K3s
-------------------------------

除了使用环境变量和 CLI 参数来配置 K3s，K3s 还可以使用配置文件。默认目录位于 `/etc/rancher/k3s/config.yaml`（或者是 `k3s.yaml` 文件）

提示

如果同时使用配置文件和 CLI 参数。 在这种情况下，值将从两个来源加载，但 CLI 参数优先级更高。 对于可重复的参数，如 --node-label，CLI 参数将覆盖列表中的所有值。

[#](#k3s-server-agent-配置) K3s Server/Agent 配置
---------------------------------------------

**`write-kubeconfig` -- 将管理客户端的 kubeconfig 写入这个文件**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  K3S_KUBECONFIG_OUTPUT=/root/.kube/config \
  sh -


```

**使用 docker 作为容器运行时**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC="--docker" \
  sh -


```

测试

```
root@cubmaster01:/workspces/runtime
CONTAINER ID   IMAGE                              COMMAND                  CREATED              STATUS              PORTS     NAMES
9e9e8cd5eb22   rancher/mirrored-library-traefik   "/entrypoint.sh --gl…"   14 seconds ago       Up 12 seconds                 k8s_traefik_traefik-bb69b68cd-ps2j6_kube-system_4905ff4a-b161-498e-b1d4-fed57a5d65a8_0
8fb6d8f2c3dc   dbd43b6716a0                       "entry"                  32 seconds ago       Up 31 seconds                 k8s_lb-tcp-443_svclb-traefik-59fdd98e-vjxlk_kube-system_4a45d7e2-5af8-4a48-99c4-1057c5ba96b0_0
6033d7ecc824   dbd43b6716a0                       "entry"                  32 seconds ago       Up 31 seconds                 k8s_lb-tcp-80_svclb-traefik-59fdd98e-vjxlk_kube-system_4a45d7e2-5af8-4a48-99c4-1057c5ba96b0_0
6b95dbafc4ee   rancher/mirrored-pause:3.6         "/pause"                 32 seconds ago       Up 31 seconds                 k8s_POD_traefik-bb69b68cd-ps2j6_kube-system_4905ff4a-b161-498e-b1d4-fed57a5d65a8_0
b13f33df38d9   rancher/mirrored-pause:3.6         "/pause"                 33 seconds ago       Up 31 seconds                 k8s_POD_svclb-traefik-59fdd98e-vjxlk_kube-system_4a45d7e2-5af8-4a48-99c4-1057c5ba96b0_0
5e8ce9d7d95e   rancher/mirrored-coredns-coredns   "/coredns -conf /etc…"   51 seconds ago       Up 50 seconds                 k8s_coredns_coredns-597584b69b-sn7n2_kube-system_d778542c-14b4-4c25-bc1f-b8b525a662a2_1
03db38691203   rancher/local-path-provisioner     "local-path-provisio…"   56 seconds ago       Up 55 seconds                 k8s_local-path-provisioner_local-path-provisioner-79f67d76f8-prcsc_kube-system_45729047-9cc5-4d39-9f9b-9c99d9dfefb7_1
9b087eb2f2d0   e57a417f15d3                       "/metrics-server --c…"   About a minute ago   Up About a minute             k8s_metrics-server_metrics-server-5c8978b444-bw47f_kube-system_b820f87d-9063-49a8-9ed9-5c501e11f88a_1
a1ead6d83564   rancher/mirrored-pause:3.6         "/pause"                 About a minute ago   Up About a minute             k8s_POD_coredns-597584b69b-sn7n2_kube-system_d778542c-14b4-4c25-bc1f-b8b525a662a2_0
7a1020d7a01f   rancher/mirrored-pause:3.6         "/pause"                 About a minute ago   Up About a minute             k8s_POD_local-path-provisioner-79f67d76f8-prcsc_kube-system_45729047-9cc5-4d39-9f9b-9c99d9dfefb7_0
cd0e940354a9   rancher/mirrored-pause:3.6         "/pause"                 About a minute ago   Up About a minute             k8s_POD_metrics-server-5c8978b444-bw47f_kube-system_b820f87d-9063-49a8-9ed9-5c501e11f88a_0



```

**针对多网卡主机安装 K3s 集群**

**k3s server:**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
INSTALL_K3S_EXEC="--node-ip=192.168.99.211" \
sh -


```

**K3s agent:**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
K3S_URL=https://192.168.99.211:6443 \
K3S_TOKEN=xiongxinwei \
INSTALL_K3S_EXEC="--node-ip=192.168.99.212" \
sh -


```

**--tls-san -- 在 TLS 证书中添加其他主机名或 IP 作为主题备用名称**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC="--tls-san 3.97.6.45"  \
  sh -


```

修改`kube-apiserver`、`kube-scheduler` 、`kube-controller-manager`、 `kube-cloud-controller-manager`、 `kubelet`、 `kube-proxy` 参数

**kubelet-arg：**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC='--kubelet-arg=max-pods=200' \
  sh -


```

**kube-apiserver：**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC='--kube-apiserver-arg=service-node-port-range=40000-50000' \
  sh -


```

**kube-proxy-arg：**

```
 curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC='--kube-proxy-arg=proxy-mode=ipvs' \
  sh -


```

**`--data-dir -- K3s` 数据存储目录，默认为 `/var/lib/rancher/k3s` 或 `${HOME}/.rancher/k3s`(如果不是 root 用户)**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC='--data-dir=/opt/k3s-data' \
  sh -


```

提示

这就有点意思了，我们默认的安装迁移了：

```
root@cubmaster01:/opt/k3s-data/server
agent  server


```

**当然，我们自己设计目录结构的时候或许可以用到~**

**禁用组件 --disable：**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC='--disable traefik' \
  sh -


```

提示

**禁用前：**

```
[root@VM-4-6-centos k3s]
ccm.yaml  coredns.yaml  local-storage.yaml  metrics-server  rolebindings.yaml  traefik.yaml


```

**禁用后：**

```
root@cubmaster01:/workspces/runtime
ccm.yaml  coredns.yaml  local-storage.yaml  metrics-server  rolebindings.yaml
root@cubmaster01:/workspces/runtime


```

**添加 label 和 taint:**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_EXEC='--node-label foo=bar,hello=world --node-taint key1=value1:NoExecute' \
  sh -


```

[#](#网络选项) 网络选项
---------------

默认情况下，`K3s` 将以 `flannel` 作为 `CNI` 运行，使用 `VXLAN` 作为默认后端。`CNI`和默认后端都可以通过参数修改。

### [#](#flannel-选项) Flannel 选项

Flannel 的默认后端是 VXLAN。要启用加密，请使用下面的 IPSec（Internet Protocol Security）或 WireGuard 选项。

<table><thead><tr><th>CLI Flag 和 Value</th><th>描述</th></tr></thead><tbody><tr><td><code>--flannel-backend=vxlan</code></td><td>(默认) 使用 VXLAN 后端。</td></tr><tr><td><code>--flannel-backend=ipsec</code></td><td>使用 IPSEC 后端，对网络流量进行加密。</td></tr><tr><td><code>--flannel-backend=host-gw</code></td><td>使用 host-gw 后端。</td></tr><tr><td><code>--flannel-backend=wireguard</code></td><td>使用 WireGuard 后端，对网络流量进行加密。可能需要额外的内核模块和配置。</td></tr></tbody></table>

### [#](#flannel-backend-使用-host-gw) flannel-backend 使用 `host-gw`

```
root@k3s1:~
        INSTALL_K3S_EXEC="--flannel-backend=host-gw" \
        INSTALL_K3S_MIRROR=cn sh -


root@k3s2:~
        INSTALL_K3S_MIRROR=cn K3S_URL=https://172.16.64.6:6443 \
        K3S_TOKEN=85892cbfef2177603f25be30344dbcd0 sh -


```

提示

```
root@k3s1:~
{
	"Network": "10.42.0.0/16",
	"Backend": {
		"Type": "host-gw"
	}
}

root@k3s1:~
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.16.64.1     0.0.0.0         UG    100    0        0 enp0s2
10.42.0.0       0.0.0.0         255.255.255.0   U     0      0        0 cni0
10.42.1.0       172.16.64.9     255.255.255.0   UG    0      0        0 enp0s2
172.16.64.0     0.0.0.0         255.255.255.0   U     0      0        0 enp0s2
172.16.64.1     0.0.0.0         255.255.255.255 UH    100    0        0 enp0s2


```

### [#](#启用-directrouting) 启用 Directrouting

当主机在同一子网时，启用 direct routes(如 host-gw)。`vxlan` 只用于将数据包封装到不同子网的主机上，同子网的主机之间使用 `host-gw`。默认值为 `false`。

```
cat >> /etc/net-conf.json <<EOF 
{
        "Network": "10.42.0.0/16",
        "Backend": {
            "Type": "vxlan",
            "Directrouting": true
	}
}
EOF


curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \
        INSTALL_K3S_EXEC="--flannel-conf=/etc/net-conf.json" \
        INSTALL_K3S_MIRROR=cn sh -


```

[#](#自定义-cni) 自定义 CNI
---------------------

使用 `--flannel-backend=none` 运行 K3s，然后在安装你选择的 CNI。

#### [#](#calico) Calico

按照 [Calico CNI 插件指南 open in new window](https://docs.projectcalico.org/master/reference/cni-plugin/configuration)。修改 Calico YAML，在 container_settings 部分中允许 IP 转发，例如：

```
"container_settings": {
              "allow_ip_forwarding": true
          }


```

> 如不配置 `"allow_ip_forwarding": true`， `svclb-traefik` 将会报错：`/usr/bin/entry: line 6: can't create /proc/sys/net/ipv4/ip_forward: Read-only file system`

```
root@k3s1:~
        INSTALL_K3S_MIRROR=cn \
        INSTALL_K3S_EXEC="--flannel-backend=none \
        --cluster-cidr=192.168.200.0/24" \
        sh -
root@k3s1:~


```

** 参考：**https://docs.projectcalico.org/getting-started/kubernetes/k3s/quickstart

[#](#使用外部数据库实现高可用安装) 使用外部数据库实现高可用安装
-----------------------------------

提示

单节点 k3s server 集群可以满足各种用例，但是对于需要 Kubernetes control-plane 稳定运行的重要环境，您可以在 HA 配置中运行 K3s。一个 K3s HA 集群由以下几个部分组成：

*   **两个或多个** `server 节点`，将为 Kubernetes API 提供服务并运行其他 control-plane 服务。
*   **零个或多个** `agent 节点`，用于运行您的应用和服务。
*   `外部数据存储` (与单个 k3s server 设置中使用的嵌入式 SQLite 数据存储相反)
*   `固定的注册地址`，位于 server 节点的前面，以允许 agent 节点向集群注册

> Agent 通过固定的注册地址进行注册，但注册后直接与其中一个 `server` 节点建立连接。这是一个由 `k3s agent` 进程发起的 `websocket` 连接，并由作为 `agent` 进程一部分运行的客户端负载均衡器维护。
> 
> **嵌入式 ETCD 使用的共识算法 Raft 建议你使用最少 3 个且数量为奇数的节点**

### [#](#环境准备) 环境准备

提醒

个人比较倾向于 etcd，Dqlite 已经被抛弃了，嵌入式 etcd 才是 yyds

使用之前的 3 台主机实验。**创建一个外部数据存储**

<table><thead><tr><th>主机名</th><th>角色</th><th>IP</th></tr></thead><tbody><tr><td>k3s-server-1</td><td>k3s master</td><td>192.168.71.130</td></tr><tr><td>k3s-server-2</td><td>k3s master</td><td>192.168.71.131</td></tr><tr><td>k3s-db</td><td>DB</td><td>121.43.165.25</td></tr><tr><td>k3s-lb</td><td>LB</td><td>172.31.13.97</td></tr><tr><td>k3s-agent</td><td>k3s agent</td><td>172.31.15.130</td></tr></tbody></table>

### [#](#外部数据库高可用) 外部数据库高可用

```
docker run --name some-mysql --restart=unless-stopped -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password -d mysql:5.7


```

**启动 k3s server 节点:**

```
curl -sfL https://get.k3s.io | sh -s - server \
  --datastore-endpoint="mysql://root:password@tcp(192.168.71.130:3306)/database-name" --tls-san 172.31.13.97


```

注意

`--tls-san`：在 TLS 证书中添加其他主机名或 IP 作为主题备用名称，本例为 LB 的 IP 否则通过 LB IP 连接 k3s api 时将会报错：`Unable to connect to the server: x509: certificate is valid for 10.43.0.1, 127.0.0.1, 172.31.2.134, 172.31.2.42, not 172.31.13.97`

`--tls-san` 可省略

验证

默认情况下，server 节点将是可调度的，因此你的工作负载可以在它们上启动。如果你希望有一个专用的 control-plane，在这个平面上不会运行用户工作负载，你可以使用 taints。`node-taint` 参数将允许你用污点配置节点，例如`--node-taint CriticalAddonsOnly=true:NoExecute`。

### [#](#agent-加入) agent 加入

Agent 节点需要一个 URL 来注册，你应该在 server 节点前面有一个稳定的 endpoint，不会随时间推移而改变。可以使用许多方法来设置此 endpoint，例如：

*   一个 4 层（TCP）负载均衡器
*   轮询 DNS
*   虚拟或弹性 IP 地址

使用 nginx 作为负载均衡器，将 6443 端口流量转发到 k3s server:

```
cat >> /etc/nginx.conf <<EOF
worker_processes 4;
worker_rlimit_nofile 40000;

events {
    worker_connections 8192;
}

stream {
    upstream k3s_api {
        least_conn;
        server 172.31.2.134:6443 max_fails=3 fail_timeout=5s;
        server 172.31.2.42:6443 max_fails=3 fail_timeout=5s;
    }
    server {
        listen     6443;
        proxy_pass k3s_api;
    }
}
EOF


```

_运行：_

```
docker run -d --restart=unless-stopped \
  -p 6443:6443 \
  -v /etc/nginx.conf:/etc/nginx/nginx.conf \
  nginx:1.14


```

### [#](#没有-cli-标志启动-agent-加入) 没有 CLI 标志启动 agent 加入

如果第一个服务器节点是在没有 `--token` CLI 标志或 `K3S_TOKEN` 变量的情况下启动的，则可以从已加入群集的任何服务器检索令牌值：

```
cat /var/lib/rancher/k3s/server/token


```

然后[可以使用令牌添加 open in new window](https://docs.k3s.io/reference/server-config#cluster-options) 其他服务器节点:

```
curl -sfL https://get.k3s.io | sh -s - server \
  \
  --datastore-endpoint="mysql://username:password@tcp(hostname:3306)/database-name"


```

提示

在 HA 集群中加入 agent 节点与在单个 server 集群中加入 agent 节点是一样的。你只需要指定 agent 应该注册到的 URL 和它应该使用的 token 即可。

```
curl -sfL https://get.k3s.io | K3S_URL=https://172.31.13.97:6443 K3S_TOKEN=mynodetoken sh -


```

有一些配置标志在所有服务器节点中必须相同：

*   网络相关标志： --cluster-dns， -`--cluster-domain`， --cluster-cidr， -`--service-cidr` `--cluster-dns``--cluster-cidr`
*   控制某些组件部署的标志：--disable-helm-controller、--`--disable-kube-proxy`、-`--disable-network-policy` 以及传递给 -`--disable` 的任何组件 `--disable-helm-controller`
*   功能相关标志：-`--secrets-encryption`

注意

确保保留此令牌的副本，因为从备份还原和添加节点时需要该副本。以前，K3s 在使用外部 SQL 数据存储时不会强制使用令牌。

[#](#嵌入式db-ha) 嵌入式 DB HA
------------------------

要在这种模式下运行 K3s，你必须有奇数的服务器节点。我们建议从三个节点开始。

要开始运行，首先启动一个服务器节点，使用 cluster-init 标志来启用集群，并使用一个标记作为共享的密钥来加入其他服务器到集群中。

```
K3S_TOKEN=SECRET k3s server --cluster-init

curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - --cluster-init


```

启动第一台服务器后，使用共享密钥将第二台和第三台服务器加入集群。

```
K3S_TOKEN=SECRET k3s server --server https://<ip or hostname of server1>:6443

curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - --server https://<ip or hostname of server1>:6443


```

查询 ETCD 集群状态：

```
ETCDCTL_ENDPOINTS='https://172.31.12.136:2379,https://172.31.4.43:2379,https://172.31.4.190:2379' 
ETCDCTL_CACERT='/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt' 
ETCDCTL_CERT='/var/lib/rancher/k3s/server/tls/etcd/server-client.crt' 
ETCDCTL_KEY='/var/lib/rancher/k3s/server/tls/etcd/server-client.key' 
ETCDCTL_API=3 etcdctl endpoint status --write-out=table


```

警告

etcd 证书默认目录：`/var/lib/rancher/k3s/server/tls/etcd` etcd 数据默认目录：`/var/lib/rancher/k3s/server/db/etcd`

[#](#集群数据存储选项) 集群数据存储选项
-----------------------

官方介绍：

使用 etcd 以外的数据存储运行 Kubernetes 的能力使 K3s 区别于其他 Kubernetes 发行版。该功能为 Kubernetes 操作者提供了灵活性。可用的数据存储选项允许您选择一个最适合您用例的数据存储。例如：

*   如果你的团队没有操作 etcd 的专业知识，可以选择 MySQL 或 PostgreSQL 等企业级 SQL 数据库。
*   如果您需要在 CI/CD 环境中运行一个简单的、短暂的集群，您可以使用嵌入式 SQLite 数据库。
*   如果你希望在边缘部署 Kubernetes，并需要一个高可用的解决方案，但又无法承担在边缘管理数据库的操作开销，你可以使用 K3s 建立在嵌入式 etcd 之上的嵌入式 HA 数据存储。

### [#](#配置参数) 配置参数

使用外部数据存储，如 PostgreSQL、MySQL 或 etcd，你必须设置`datastore-endpoint`参数，以便 K3s 知道如何连接到它。你也可以指定参数来配置连接的认证和加密。下表参数，它们可以作为 CLI 标志或环境变量传递。

<table><thead><tr><th>CLI Flag</th><th>环境变量</th><th>描述</th></tr></thead><tbody><tr><td><code>--datastore-endpoint</code></td><td><code>K3S_DATASTORE_ENDPOINT</code></td><td>指定一个 PostgresSQL、MySQL 或 etcd 连接字符串。用于描述与数据存储的连接。这个字符串的结构是特定于每个后端的，详情如下。</td></tr><tr><td><code>--datastore-cafile</code></td><td><code>K3S_DATASTORE_CAFILE</code></td><td>TLS 证书颁发机构（CA）文件，用于帮助确保与数据存储的通信安全。如果你的数据存储通过 TLS 服务请求，使用由自定义证书颁发机构签署的证书，你可以使用这个参数指定该 CA，这样 K3s 客户端就可以正确验证证书。</td></tr><tr><td><code>--datastore-certfile</code></td><td><code>K3S_DATASTORE_CERTFILE</code></td><td>TLS 证书文件，用于对数据存储进行基于客户端证书的验证。要使用这个功能，你的数据存储必须被配置为支持基于客户端证书的认证。如果你指定了这个参数，你还必须指定<code>datastore-keyfile</code>参数。</td></tr><tr><td><code>--datastore-keyfile</code></td><td><code>K3S_DATASTORE_KEYFILE</code></td><td>TLS 密钥文件，用于对数据存储进行基于客户端证书的认证。更多细节请参见前面的<code>datastore-certfile</code>参数。</td></tr></tbody></table>

作为最佳实践，我们建议将这些参数设置为环境变量，而不是命令行参数，这样你的数据库证书或其他敏感信息就不会作为进程信息的一部分暴露出来。

官方建议

作为最佳实践，我们建议将这些参数设置为环境变量，而不是命令行参数，这样你的数据库证书或其他敏感信息就不会作为 **进程信息** 的一部分暴露出来。

[#](#私有仓库) 私有仓库
---------------

可以将 Containerd 配置为连接到私有注册表，并使用它们在节点上拉取私有映像。

K3s 默认使用 containerd 作为容器运行时，所以在 docker 上配置镜像仓库是不生效的

K3s registry 配置目录为： `/etc/rancher/k3s/registries.yaml`。K3s 启动时，K3s 会检查 `/etc/rancher/k3s/` 中是否存在 `registries.yaml` 文件，并指示 `containerd` 使用文件中定义的镜像仓库。如果你想使用一个私有的镜像仓库，那么你需要在每个使用镜像仓库的节点上以 root 身份创建这个文件。

请注意，server 节点默认是可以调度的。如果你没有在 server 节点上设置污点，那么将在它们上运行工作负载，请确保在每个 server 节点上创建 `registries.yaml` 文件。

### [#](#registries-yaml-文件) registries.yaml 文件

**组成部分：**

1.  `mirrors`：定义专用注册表的名称和端点的指令
2.  `configs`：`configs`部分定义每个镜像的 TLS 和凭据配置。对于每个镜像，您可以定义`auth`和 / 或 `tls`.

提示

containerd 使用了类似 K8S 中 svc 与 endpoint 的概念，svc 可以理解为 **访问名称**，这个名称会解析到对应的 endpoint 上。 也可以理解 mirror 配置就是一个反向代理，它把客户端的请求代理到 endpoint 配置的后端镜像仓库。mirror 名称可以随意填写，但是必须符合 IP 或域名的定义规则。并且可以配置多个 endpoint，默认解析到第一个 endpoint，如果第一个 endpoint 没有返回数据，则自动切换到第二个 endpoint，以此类推。`INSTALL_K3S_MIRROR=cn`

**💡简单的一个案例如下：**

```
mirrors:
  "172.31.6.200:5000":
    endpoint:
      - "http://172.31.6.200:5000"
      - "http://x.x.x.x:5000"
      - "http://y.y.y.y:5000"
  "rancher.ksd.top:5000":
    endpoint:
      - "http://172.31.6.200:5000"
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"

configs:
  "172.31.6.200:5000":
    auth:
      username: admin
      password: Harbor@12345
    tls:
      cert_file: /home/ubuntu/harbor2.kingsd.top.cert
      key_file:  /home/ubuntu/harbor2.kingsd.top.key
      ca_file:   /home/ubuntu/ca.crt


```

可以通过 `crictl pull 172.31.6.200:5000/library/alpine` 和 `crictl pull rancher.ksd.top:5000/library/alpine` 获取到镜像，但镜像都是从同一个仓库获取到的。

#### [#](#mirrors) mirrors

```
mirrors:
  mycustomreg.com:
    endpoint:
      - "https://mycustomreg.com:5000"


```

**Rewrites:**

每个镜像都可以有一组 `Rewrites`。`Rewrites` 可以根据正则表达式更改图像的标签。如果镜像注册表中的组织 / 项目结构与上游结构不同，这将非常有用。

例如，以下配置将以透明方式从 `registry.example.com:5000/mirrorproject/rancher-images/coredns-coredns:1.6.3` 拉取映像 `docker.io/rancher/coredns-coredns:1.6.3`:

```
mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
    rewrite:
      "^rancher/(.*)": "mirrorproject/rancher-images/$1"


```

> 映像仍将存储在原始名称下，以便 `crictl image ls` 将显示节点上可用的 `docker.io/rancher/coredns-coredns:1.6.3`，即使映像是从具有不同名称的镜像注册表中提取的。

#### [#](#configs) configs

`configs`部分定义每个镜像的 TLS 和凭据配置。对于每个镜像，您可以定义`auth`和 / 或 `tls`.

`tls` 部分包括：

<table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody><tr><td><code>cert_file</code></td><td>将用于向注册表进行身份验证的客户端证书路径</td></tr><tr><td><code>key_file</code></td><td>将用于向注册表进行身份验证的客户端密钥路径</td></tr><tr><td><code>ca_file</code></td><td>定义用于验证注册表的服务器证书文件的 CA 证书路径</td></tr><tr><td><code>insecure_skip_verify</code></td><td>定义是否应跳过注册表的 TLS 验证的布尔值</td></tr></tbody></table>

身份验证部分由用户名 / 密码或`auth`令牌组成：

<table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody><tr><td><code>username</code></td><td>专用注册表基本身份验证的用户名</td></tr><tr><td><code>password</code></td><td>专用注册表基本身份验证的用户密码</td></tr><tr><td><code>auth</code></td><td>专用注册表基本身份验证的身份验证令牌</td></tr></tbody></table>

#### [#](#使用-tls) 使用 TLS

以下示例显示了使用 TLS 时如何在每个节点上配置 /`/etc/rancher/k3s/registries.yaml`。

```
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "harbor.kingsd.top":
    endpoint:
      - "https://harbor.kingsd.top"
configs:
  "harbor.kingsd.top":
    auth:
      username: admin
      password: Harbor@12345
EOF
systemctl restart k3s


```

**自签名证书:**

```
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "harbor2.kingsd.top":
    endpoint:
      - "https://harbor2.kingsd.top"
configs:
  "harbor2.kingsd.top":
    auth:
      username: admin
      password: Harbor@12345
    tls:
      cert_file: /home/ubuntu/harbor2.kingsd.top.cert
      key_file:  /home/ubuntu/harbor2.kingsd.top.key
      ca_file:   /home/ubuntu/ca.crt
EOF
systemctl restart k3s


```

**不使用 TLS (Http registry):**

> 在没有 TLS 通信的情况下，需要为 endpoints 指定 http://，否则将默认为 https

```
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "172.31.19.227:5000":
    endpoint:
      - "http://172.31.19.227:5000"
EOF
systemctl restart k3s


```

**带身份验证的和不带身份验证的（auth)**

```
mirrors:
  docker.io:
    endpoint:
      - "https://mycustomreg.com:5000"
configs:
  "mycustomreg:5000":
    auth:
      username: xxxxxx 
      password: xxxxxx 
    tls:
      cert_file: 
      key_file:  
      ca_file:   


```

```
mirrors:
  docker.io:
    endpoint:
      - "https://mycustomreg.com:5000"
configs:
  "mycustomreg:5000":
    tls:
      cert_file: 
      key_file:  
      ca_file:   


```

**配置 Mirror:**

```
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"
EOF
systemctl restart k3s


```

完整示例

```
mirrors:
  "harbor.kingsd.top":
    endpoint:
      - "https://harbor.kingsd.top"
  "harbor2.kingsd.top":
    endpoint:
      - "https://harbor2.kingsd.top"
  "172.31.19.227:5000":
    endpoint:
      - "http://172.31.19.227:5000"
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"

configs:
  "harbor.kingsd.top":
    auth:
      username: admin
      password: Harbor@12345

  "harbor2.kingsd.top":
    auth:
      username: admin
      password: Harbor@12345
    tls:
      cert_file: /home/ubuntu/harbor2.kingsd.top.cert
      key_file:  /home/ubuntu/harbor2.kingsd.top.key
      ca_file:   /home/ubuntu/ca.crt



``` 

### [#](#配置-containerd) 配置 Containerd

K3s 将会在`/var/lib/rancher/k3s/agent/etc/containerd/config.toml`中为 containerd 生成 `config.toml`。

如果要对这个文件进行高级设置，你可以在同一目录中创建另一个名为 `config.toml.tmpl` 的文件，此文件将会代替默认设置。

`config.toml.tmpl`将被视为 Go 模板文件，并且`config.Node`结构被传递给模板。截取下面模板示例介绍了如何使用结构来自定义配置文件。

```
package templates

import (
	"github.com/rancher/wharfie/pkg/registries"

	"github.com/k3s-io/k3s/pkg/daemons/config"
)

type ContainerdRuntimeConfig struct {
	RuntimeType string
	BinaryName  string
}

type ContainerdConfig struct {
	NodeConfig            *config.Node
	DisableCgroup         bool
	SystemdCgroup         bool
	IsRunningInUserNS     bool
	EnableUnprivileged    bool
	PrivateRegistryConfig *registries.Registry
	ExtraRuntimes         map[string]ContainerdRuntimeConfig
}


```

### [#](#将映像添加到专用注册表) 将映像添加到专用注册表

首先，从 GitHub 获取您正在使用的版本. txt k3s-images 文件。从 docker.io 中提取 k3s 映像. txt 文件中列出的 K3s 映像

示例：`docker pull docker.io/rancher/coredns-coredns:1.6.3`

然后，将映像重新标记到专用注册表。

示例：docker tag coredns-coredns：`docker tag coredns-coredns:1.6.3 mycustomreg:5000/coredns-coredns`

最后，将映像推送到专用注册表。

示例：`docker push mycustomreg.com:5000/coredns-coredns`

[#](#离线安装) 离线安装
---------------

离线安装的过程主要分为以下两个步骤：

**步骤 1**：部署镜像，本文提供了两种部署方式，分别是**部署私有镜像仓库**和**手动部署镜像**。请在这两种方式中选择一种执行。

**步骤 2**：安装 K3s，本文提供了两种安装方式，分别是**单节点安装**和**高可用安装**。完成镜像部署后，请在这两种方式中选择一种执行。

**离线升级 K3s 版本**：完成离线安装 K3s 后，您还可以通过脚本升级 K3s 版本，或启用自动升级功能，以保持离线环境中的 K3s 版本与最新的 K3s 版本同步。

### [#](#通过私有镜像仓库安装-k3s) 通过私有镜像仓库安装 K3s

**将所需镜像上传到私有镜像仓库：**

K3s 镜像列表可以从 https://github.com/k3s-io/k3s/releases 获取。

**创建镜像仓库 YAML：**

按照[私有镜像仓库配置指南 open in new window](http://docs.rancher.cn/docs/k3s/installation/private-registry/_index/) 创建并配置`registry.yaml`文件。

```
mkdir -p /etc/rancher/k3s/
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "docker.io":
    endpoint:
      - "https://harbor.kingsd.top"
configs:
  "docker.io":
    auth:
      username: admin
      password: Harbor@12345
EOF


```

**安装单节点 K3s：**

1.  从 [K3s GitHub Releaseopen in new window](https://github.com/rancher/k3s/releases) 页面获取 K3s 二进制文件，K3s 二进制文件需要与离线镜像的版本匹配。
    
2.  获取 K3s 安装脚本：https://get.k3s.io。
    
3.  将二进制文件放在每个节点的`/usr/local/bin`中，并确保拥有可执行权限。将安装脚本放在每个节点的任意位置，并将其命名为`install.sh`。
    
4.  安装 K3s server：
    

```
INSTALL_K3S_SKIP_DOWNLOAD=true ./install.sh


```

5.  将 agent 加入到 K3s 集群

```
INSTALL_K3S_SKIP_DOWNLOAD=true K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken ./install.sh


```

**通过手动部署镜像安装 K3s：**

请按照以下步骤准备镜像和 K3s 二进制文件：

1.  从 [K3s GitHub Releaseopen in new window](https://github.com/rancher/k3s/releases) 页面获取你所运行的 K3s 版本的镜像 tar 文件。
    
2.  将 tar 文件放在`images`目录下，例如：
    

```
sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp ./k3s-airgap-images-$ARCH.tar /var/lib/rancher/k3s/agent/images/


```

3.  将 k3s 二进制文件放在 `/usr/local/bin/k3s`路径上，并确保拥有可执行权限。
    
4.  安装 K3s server：
    

```
INSTALL_K3S_SKIP_DOWNLOAD=true ./install.sh


```

5.  将 agent 加入到 K3s 集群

```
INSTALL_K3S_SKIP_DOWNLOAD=true K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken ./install.sh


```

指定`INSTALL_K3S_SKIP_DOWNLOAD=true`参数指定使用本地 K3s 二进制文件进行安装。

```
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server --datastore-endpoint=mysql://username:password@tcp(hostname:3306)/database-name' ./install.sh


```

[#](#升级-k3s) 升级 K3s
-------------------

### [#](#通过脚本升级) 通过脚本升级

离线环境的升级可以通过以下步骤完成：

1.  从 [K3s GitHub Releaseopen in new window](https://github.com/rancher/k3s/releases) 页面下载要升级到的 K3s 版本。将 tar 文件放在每个节点的`/var/lib/rancher/k3s/agent/images/`目录下。删除旧的 tar 文件。
2.  复制并替换每个节点上`/usr/local/bin`中的旧 K3s 二进制文件。复制 https://get.k3s.io 的安装脚本（因为它可能在上次发布后发生了变化）。再次运行脚本。
3.  重启 K3s 服务。

### [#](#在线脚本升级) 在线脚本升级

在线脚本升级

当升级 K3s 时，K3s 服务会重启或停止，但 K3s 容器会继续运行。 要停止所有的 K3s 容器并重置容器的状态，可以使用 `k3s-killall.sh` 脚本。 killall 脚本清理容器、K3s 目录和网络组件，同时也删除了 iptables 链和所有相关规则。集群数据不会被删除。

你可以通过使用安装脚本升级 K3s，或者手动安装所需版本的二进制文件。

> 注意： 升级时，先逐个升级 server 节点，然后再升级其他 agent 节点。

### [#](#channels-说明) Channels 说明

通过安装脚本或使用我们的[自动升级 open in new window](http://docs.rancher.cn/docs/k3s/upgrades/basic/_index) 功能进行的升级可以绑定到不同的发布 channels。以下是可用的 channels。

<table><thead><tr><th>CHANNEL</th><th>描 述</th></tr></thead><tbody><tr><td>stable</td><td>(默认) 稳定版建议用于生产环境。这些版本已经过一段时间的社区强化。</td></tr><tr><td>latest</td><td>推荐使用最新版本尝试最新的功能。 这些版本还没有经过社区强化。</td></tr><tr><td>v1.19 (例子)</td><td>每一个支持的 Kubernetes 次要版本都有一个发布 channel，它们分别是<code>v1.19</code>、<code>v1.20</code>和<code>v1.21</code>。。这些 channel 会选择最新的可用补丁版本，不一定是稳定版本。</td></tr></tbody></table>

对于详细的最新 channels 列表，您可以访问 [k3s channel 服务 APIopen in new window](https://update.k3s.io/v1-release/channels)。关于 channels 工作的更多技术细节，请参见 [channelserver 项目 open in new window](https://github.com/rancher/channelserver)。

### [#](#使用安装脚本升级-k3s) 使用安装脚本升级 K3s

要从旧版本升级 K3s，你可以使用 **相同的标志** 重新运行安装脚本:

*   升级到最新 `stable` 版本

```
curl -sfL https://get.k3s.io | sh -


```

*   升级到 `latest` 版本

```
curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -


```

*   升级到 `v1.20` 的最新版本

```
curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL="v1.20" sh -


```

*   升级到指定版本

```
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -


```

#### [#](#使用二进制文件手动升级-k3s) 使用二进制文件手动升级 K3s

1.  从[发布 open in new window](https://github.com/rancher/k3s/releases) 下载所需版本的 K3s 二进制文件
2.  将下载的二进制文件复制到`/usr/local/bin/k3s`（或您所需的位置）
3.  停止旧的 K3s 二进制文件
4.  启动新的 K3s 二进制文件

### [#](#自动升级) 自动升级

> 注意： 此功能从 v1.17.4+k3s1 开始提供支持。

你可以使用 Rancher 的 system-upgrad-controller 来管理 K3s 集群升级。这是一种 Kubernetes 原生的集群升级方法。它利用[自定义资源定义 (CRD)open in new window](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-resources)、`计划`和[控制器 open in new window](https://kubernetes.io/docs/concepts/architecture/controller/)，根据配置的计划安排升级。

控制器通过监控计划和选择要在其上运行升级 [jobopen in new window](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/) 的节点来调度升级。计划通过[标签选择器 open in new window](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) 定义哪些节点应该升级。当一个 job 成功运行完成后，控制器会给它运行的节点打上相应的标签。

关于 system-upgrade-controller 的设计和架构或其与 K3s 集成的更多细节，请参见以下 Git 仓库：

*   [system-upgrade-controlleropen in new window](https://github.com/rancher/system-upgrade-controller)
*   [k3s-upgradeopen in new window](https://github.com/rancher/k3s-upgrade)

要以这种方式进行自动升级，你必须：

1.  将 system-upgrade-controller 安装到您的集群中
2.  配置计划

#### [#](#安装-system-upgrade-controller) 安装 system-upgrade-controller

System-upgrade-controller 可以作为 deployment 安装到您的集群中。Deployment 需要一个 service-account、clusterRoleBinding 和一个 configmap。要安装这些组件，请运行以下命令:

```
kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.6.2/system-upgrade-controller.yaml


```

#### [#](#配置计划) 配置计划

建议您最少创建两个计划：升级 server（master）节点的计划和升级 agent（worker）节点的计划。根据需要，您可以创建其他计划来控制跨节点的滚动升级。以下两个示例计划将把您的集群升级到 K3s v1.20.4+k3s1。创建计划后，控制器将接收这些计划并开始升级您的集群。

```
# Server plan
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: server-plan
  namespace: system-upgrade
spec:
  concurrency: 1
  cordon: true
  nodeSelector:
    matchExpressions:
    - key: node-role.kubernetes.io/master
      operator: In
      values:
      - "true"
  serviceAccountName: system-upgrade
  upgrade:
    image: rancher/k3s-upgrade
  version: v1.20.4+k3s1

---
# Agent plan
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: agent-plan
  namespace: system-upgrade
spec:
  concurrency: 1
  cordon: true
  nodeSelector:
    matchExpressions:
    - key: node-role.kubernetes.io/master
      operator: DoesNotExist
  prepare:
    args:
    - prepare
    - server-plan
    image: rancher/k3s-upgrade
  serviceAccountName: system-upgrade
  upgrade:
    image: rancher/k3s-upgrade
  version: v1.20.4+k3s1


```

关于这些计划，有几个重要的事情需要提醒：

首先，必须在部署控制器的同一命名空间中创建计划。

其次，`concurrency`字段表示可以同时升级多少个节点。

第三，`server-plan`通过指定一个标签选择器来选择带有`node-role.kubernetes.io/master`标签的节点，从而锁定 server 节点。`agent-plan`通过指定一个标签选择器来选择没有该标签的节点，以 agent 节点为目标。

第四，`agent-plan`中的 `prepare` 步骤会使该计划等待`server-plan`完成后再执行升级 jobs。

第五，两个计划的`version`字段都设置为 v1.17.4+k3s1。或者，你可以省略 `version` 字段，将 `channel` 字段设置为解析到 K3s 版本的 URL。这将导致控制器监控该 URL，并在它解析到新版本时随时升级集群。这与 [release channels](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/docs/k3s/upgrades/basic/_index#%E5%8F%91%E5%B8%83-channels) 配合得很好。因此，你可以用下面的 channel 配置你的计划，以确保你的集群总是自动升级到 K3s 的最新稳定版本。

```
apiVersion: upgrade.cattle.io/v1
kind: Plan
...
spec:
  ...
  channel: https://update.k3s.io/v1-release/channels/stable



```

如上所述，一旦控制器检测到计划已创建，升级就会立即开始。更新计划将使控制器重新评估计划并确定是否需要再次升级。

您可以通过 kubectl 查看 plans 和 jobs 来监控升级的进度：

```
kubectl -n system-upgrade get plans -o yaml
kubectl -n system-upgrade get jobs -o yaml


```

[#](#连接到-k3s-kubernets-集群的三种方式) 连接到 k3s kubernets 集群的三种方式
---------------------------------------------------------

### [#](#kubeconfig) kubeconfig

在 K3s 安装过程中，**kubeconfig** 文件被写入 `/etc/rancher/k3s/k3s.yaml`。您将需要此文件才能使用首选的 Kubernetes 客户端连接到您的集群。你的 kubeconfig 会喜欢这样：

```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0...
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: default
  user:
    client-certificate-data: LS...
    client-key-data: LS...


```

**出于好奇，我们可以检查 k3s-agent 单元文件如何使用环境文件 ( `/etc/systemd/system/k3s-agent.service.env` ) 来存储变量变量 K3S_URL 和 K3S_TOKEN：**

```
root@cubnode02:/workspces/runtime
[Unit]
Description=Lightweight Kubernetes
Documentation=https://k3s.io
Wants=network-online.target
After=network-online.target

[Install]
WantedBy=multi-user.target

[Service]
Type=notify
EnvironmentFile=-/etc/default/%N
EnvironmentFile=-/etc/sysconfig/%N
EnvironmentFile=-/etc/systemd/system/k3s-agent.service.env
KillMode=process
Delegate=yes


LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
TimeoutStartSec=0
Restart=always
RestartSec=5s
ExecStartPre=/bin/sh -xc '! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service'
ExecStartPre=-/sbin/modprobe br_netfilter
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/k3s \
    agent \
	'--server' \



```

提示

如果不依赖 systemctl 服务进行管理，只能 nohup 自己搞，要么写程序的时候做一个进程或者线程 deamon 守护，一直让它持续跑。 ExecStart 几个段，和你平时执行的一句话命令是一样的 只不过写成了配置文件，交给 systemctl 管理整一个服务 environmentFile 相当于你每次执行的手工环境变量全整好了

`nohup 命令 &`

**k3s-agent.service.env** 的内容如下所示：

```
root@cubnode02:/workspces/runtime
K3S_TOKEN='SECRET'
K3S_URL='https://192.168.71.130:6443'


```

要手动启动`k3s` 代理，我们还可以使用环境变量的选项`--server`和`--token inetad`：

```
k3s agent --server https://192.168.71.130:6443 --token "K10b625ace11027708856a6369064ef7cbd8e695a65457c9815e5f7ec2c3eca0635::server:SECRET"


```

### [#](#kubectl) kubectl

连接到 Kubernetes 集群的最常见方法是 `'kubectl'` 命令。该命令在 K3s 安装期间自动安装在群集节点上。

要连接到集群，你必须让 `kubectl` 知道在哪里可以找到 `kubeconfig`。您可以通过 **将 kubeconfig 文件指定为选项、设置环境变量或将其复制到 `~/.kube/config`** 来做到这一点——任何更适合您的方法：

```
kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml  get nodes



export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
kubectl get nodes



cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
kubectl get nodes



```

有了 kubeconfig，你可以尝试以下两个命令。

```
NAME          STATUS   ROLES                       AGE   VERSION
cubmaster01   Ready    control-plane,etcd,master   15m   v1.25.4+k3s1

NAMESPACE     NAME                                      READY   STATUS      RESTARTS   AGE
kube-system   coredns-597584b69b-rb9bh                  1/1     Running     0          15m
kube-system   helm-install-traefik-crd-9h7rn            0/1     Completed   0          15m
kube-system   helm-install-traefik-fn7n7                0/1     Completed   1          15m
kube-system   local-path-provisioner-79f67d76f8-v5nxz   1/1     Running     0          15m
kube-system   metrics-server-5c8978b444-xbdbk           1/1     Running     0          15m
kube-system   svclb-traefik-f1a0eea9-ch5hp              2/2     Running     0          14m
kube-system   traefik-bb69b68cd-wzt8q                   1/1     Running     0          14m


```

### [#](#lens-kubernetes-ide) Lens Kubernetes IDE

如果您更喜欢图形界面来管理您的集群，您绝对应该查看 [Lensopen in new window](https://k8slens.dev/)。它确实可以更轻松地管理工作负载，配置映射，机密，服务，入口，并为您提供正在发生的事情的概述。最好的事情是，它是完全免费和开源的。

> “Lens 为 Kubernetes 中运行的所有内容提供了完整的态势感知。它降低了刚起步的人的进入门槛，并从根本上提高了拥有更多经验的人的生产力。
> 
> https://github.com/lensapp/lens/

[Kubenavopen in new window](https://kubenav.io/) 也是一个图形化的 Kubernetes 工具。Kubenav 最好的一点是它可用于移动 Android 和 iOS 设备，因此您可以在旅途中管理您的 Kubernetes 集群;)。Kubenav 也是开源的，可在 iOS App Store 和 Play Store 上使用。您也可以从 [Github 存储库 open in new window](https://github.com/kubenav/kubenav/) 下载桌面版本.

安装后导航到 “群集”，然后按加号添加群集。向下滚动到 “导入 Kubeconfig”，将内容粘贴到文本字段中，然后按 “添加”。

![](https://sm.nsddd.top/smimage-20221127124516263.png)

注意：Kubernetes API 端口 （TCP/6443） 必须可用于您的手机。如果您不想打开该端口到互联网，您可以通过 VPN 连接到集群。请参阅我在 [Ubuntu 上设置 Wireguard 的指南 open in new window](https://headworq.org/en-how-to-install-wiregurad-on-ubuntu/).

[#](#end-链接) END 链接
-------------------

*   [Ⓜ️回到目录🏠](chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/)
    
*   [**🫵参与贡献💞❤️‍🔥💖**open in new window](https://nsddd.top/archives/contributors))
    
*   ✴️版权声明 © ：本书所有内容遵循 [CC-BY-SA 3.0 协议（署名 - 相同方式共享）©open in new window](http://zh.wikipedia.org/wiki/Wikipedia:CC-by-sa-3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC)